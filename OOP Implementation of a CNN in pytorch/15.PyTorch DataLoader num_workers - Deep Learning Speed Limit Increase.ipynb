{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599454305792",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_workers optional attribute - data loader class\n",
    "\n",
    "#The num_workers attribute tells the data loader instance how many sub-processes to use for data loading. By default, the num_workers value is set to zero, and a value of zero tells the loader to load the data inside the main process.\n",
    "\n",
    "#This means that the training process will work sequentially inside the main process. After a batch is used during the training process and another one is needed, we read the batch data from disk.\n",
    "\n",
    "#Now, if we have a worker process, we can make use of the fact that our machine has multiple cores. This means that the next batch can already be loaded and ready to go by the time the main process is ready for another batch. This is where the speed up comes from. The batches are loaded using additional worker processes and are queued up in memory.\n",
    "\n",
    "#num_workers(int, optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "torch.set_printoptions(linewidth = 120) \n",
    "torch.set_grad_enabled(True) \n",
    "\n",
    "from itertools import product #computes cartesian product given multiple list inputs  \n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter #allows to send data to tensorboard files \n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "\n",
    "from collections import OrderedDict\n",
    "from collections import namedtuple\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunBuilder():\n",
    "    @staticmethod #The main thing to note about using this class is that it has a static method called get_runs(). This method will get the runs for us that it builds based on the parameters we pass in.\n",
    "    def get_runs(params):\n",
    "\n",
    "        Run = namedtuple('Run', params.keys())\n",
    "\n",
    "        runs = []\n",
    "        for v in product(*params.values()):\n",
    "            runs.append(Run(*v))\n",
    "\n",
    "        return runs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "        \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "\n",
    "\n",
    "    def forward(self, t):\n",
    "        \n",
    "        # (1) input layer\n",
    "        t = t\n",
    "        \n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size = 2, stride =2)\n",
    "\n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size = 2, stride =2)\n",
    "\n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 12*4*4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "       \n",
    "        # (5) hidden linear layer\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t) \n",
    "\n",
    "        # (6) ouput layer\n",
    "        t = self.out(t)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='./data'  \n",
    "    ,train=True    \n",
    "    ,download=True \n",
    "    ,transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunManager():\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.epoch_count = 0\n",
    "        self.epoch_loss = 0\n",
    "        self.epoch_num_correct = 0\n",
    "        self.epoch_start_time = None\n",
    "\n",
    "        self.run_params = None \n",
    "        self.run_count = 0\n",
    "        self.run_data = []  #track param,results for each epoch for each run\n",
    "        self.run_start_time = None  #run durations\n",
    "\n",
    "        self.network = None #save network\n",
    "        self.loader = None #save data loader for run\n",
    "        self.tb = None #summarywriter for tensorboard\n",
    "\n",
    "\n",
    "#Anytime we see this, we need to be thinking about removing these prefixes. (epoch_count, epoch_loss, ....)\n",
    "# Data that belongs together should be together. \n",
    "# This is done by encapsulating the data inside of a class.\n",
    "#this is done in next cell\n",
    "\n",
    "    def begin_run(self, run, network, loader):\n",
    "\n",
    "        self.run_start_time = time.time()\n",
    "\n",
    "        self.run_params = run\n",
    "        self.run_count += 1\n",
    "\n",
    "        self.network = network\n",
    "        self.loader = loader\n",
    "        self.tb = SummaryWriter(comment=f'-{run}')\n",
    "\n",
    "        images, labels = next(iter(self.loader))\n",
    "        grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "        self.tb.add_image('images', grid)\n",
    "        self.tb.add_graph(self.network, images)    \n",
    "\n",
    "    def end_run(self):\n",
    "        self.tb.close()\n",
    "        self.epoch_count = 0\n",
    "\n",
    "    def begin_epoch(self):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "        self.epoch_count += 1\n",
    "        self.epoch_loss = 0\n",
    "        self.epoch_num_correct = 0\n",
    "\n",
    "    def end_epoch(self):\n",
    "        epoch_duration = time.time() - self.epoch_start_time\n",
    "        run_duration = time.time() - self.run_start_time\n",
    "\n",
    "        loss = self.epoch_loss / len(self.loader.dataset)\n",
    "        accuracy = self.epoch_num_correct / len(self.loader.dataset)\n",
    "\n",
    "        self.tb.add_scalar('Loss', loss, self.epoch_count)\n",
    "        self.tb.add_scalar('Accuracy', accuracy, self.epoch_count)\n",
    "\n",
    "        for name, param in self.network.named_parameters():\n",
    "            self.tb.add_histogram(name, param, self.epoch_count)\n",
    "            self.tb.add_histogram(f'{name}.grad', param.grad, self.epoch_count)\n",
    "\n",
    "        results = OrderedDict()\n",
    "        results['run'] = self.run_count\n",
    "        results['epoch'] = self.epoch_count\n",
    "        results['loss'] = loss\n",
    "        results['accuracy'] = accuracy\n",
    "        results['epoch duration'] = epoch_duration\n",
    "        results['run duration'] = run_duration\n",
    "        for k,v in self.run_params._asdict().items(): results[k] = v\n",
    "        self.run_data.append(results)\n",
    "        df = pd.DataFrame.from_dict(self.run_data, orient='columns')\n",
    "        \n",
    "        clear_output(wait=True) #specific to jy notebooks clear curr o/p and display new data frame\n",
    "        display(df)\n",
    "    \n",
    "    def track_loss(self, loss):\n",
    "        self.epoch_loss += loss.item() * batch[0].shape[0]\n",
    "\n",
    "    def track_num_correct(self, preds, labels):\n",
    "        self.epoch_num_correct += self._get_num_correct(preds, labels)\n",
    "    \n",
    "    def _get_num_correct(self, preds, labels):\n",
    "        return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "    @torch.no_grad()\n",
    "\n",
    "    def _get_num_correct(self, preds, labels):\n",
    "        return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "    \n",
    "    def save(self, fileName):\n",
    "\n",
    "        pd.DataFrame.from_dict(self.run_data, orient='columns').to_csv(f'{fileName}.csv')\n",
    "        with open(f'{fileName}.json', 'w', encoding='utf-8') as f:json.dump(self.run_data, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract a class - refactoring techniques \n",
    "\n",
    "class Epoch():\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.loss = 0\n",
    "        self.num_correct = 0\n",
    "        self.start_time = None \n",
    "\n",
    "#Then, we'll replace these class variable with an instance of the Epoch class.\n",
    "#We might even change the count variable to have a more intuitive name, like say number or id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "    run  epoch      loss  accuracy  epoch duration  run duration    lr  \\\n0     1      1  0.561366  0.786517       15.317893     15.589547  0.01   \n1     2      1  0.571351  0.784783       14.114507     15.360426  0.01   \n2     3      1  0.563658  0.786100       14.080857     15.898442  0.01   \n3     4      1  0.556113  0.788850       13.930343     16.026564  0.01   \n4     5      1  0.601789  0.771533       15.790455     18.520335  0.01   \n5     6      1  0.550829  0.795100       21.496845     28.202021  0.01   \n6     7      1  0.999788  0.615267       15.657587     16.579581  0.01   \n7     8      1  0.897529  0.659667       11.357492     13.611151  0.01   \n8     9      1  1.073060  0.580150       11.718287     13.906860  0.01   \n9    10      1  0.948047  0.650517       12.759156     15.359352  0.01   \n10   11      1  1.101623  0.584983       11.533110     16.402203  0.01   \n11   12      1  0.986539  0.630117       19.376685     27.088447  0.01   \n12   13      1  2.142883  0.241983       12.733782     19.742868  0.01   \n13   14      1  2.226118  0.209117       10.196291     19.131536  0.01   \n14   15      1  2.146939  0.198500        9.796244     18.588745  0.01   \n15   16      1  2.129873  0.203933        9.587353     17.945299  0.01   \n16   17      1  2.142754  0.208950       10.874910     21.091576  0.01   \n17   18      1  2.131707  0.229233       14.592563     28.288604  0.01   \n\n    batch_size  num_workers  \n0          100            0  \n1          100            1  \n2          100            2  \n3          100            4  \n4          100            8  \n5          100           16  \n6         1000            0  \n7         1000            1  \n8         1000            2  \n9         1000            4  \n10        1000            8  \n11        1000           16  \n12       10000            0  \n13       10000            1  \n14       10000            2  \n15       10000            4  \n16       10000            8  \n17       10000           16  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>run</th>\n      <th>epoch</th>\n      <th>loss</th>\n      <th>accuracy</th>\n      <th>epoch duration</th>\n      <th>run duration</th>\n      <th>lr</th>\n      <th>batch_size</th>\n      <th>num_workers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0.561366</td>\n      <td>0.786517</td>\n      <td>15.317893</td>\n      <td>15.589547</td>\n      <td>0.01</td>\n      <td>100</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>0.571351</td>\n      <td>0.784783</td>\n      <td>14.114507</td>\n      <td>15.360426</td>\n      <td>0.01</td>\n      <td>100</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>0.563658</td>\n      <td>0.786100</td>\n      <td>14.080857</td>\n      <td>15.898442</td>\n      <td>0.01</td>\n      <td>100</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>0.556113</td>\n      <td>0.788850</td>\n      <td>13.930343</td>\n      <td>16.026564</td>\n      <td>0.01</td>\n      <td>100</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>0.601789</td>\n      <td>0.771533</td>\n      <td>15.790455</td>\n      <td>18.520335</td>\n      <td>0.01</td>\n      <td>100</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>1</td>\n      <td>0.550829</td>\n      <td>0.795100</td>\n      <td>21.496845</td>\n      <td>28.202021</td>\n      <td>0.01</td>\n      <td>100</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>1</td>\n      <td>0.999788</td>\n      <td>0.615267</td>\n      <td>15.657587</td>\n      <td>16.579581</td>\n      <td>0.01</td>\n      <td>1000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>1</td>\n      <td>0.897529</td>\n      <td>0.659667</td>\n      <td>11.357492</td>\n      <td>13.611151</td>\n      <td>0.01</td>\n      <td>1000</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>1</td>\n      <td>1.073060</td>\n      <td>0.580150</td>\n      <td>11.718287</td>\n      <td>13.906860</td>\n      <td>0.01</td>\n      <td>1000</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>1</td>\n      <td>0.948047</td>\n      <td>0.650517</td>\n      <td>12.759156</td>\n      <td>15.359352</td>\n      <td>0.01</td>\n      <td>1000</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>11</td>\n      <td>1</td>\n      <td>1.101623</td>\n      <td>0.584983</td>\n      <td>11.533110</td>\n      <td>16.402203</td>\n      <td>0.01</td>\n      <td>1000</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>12</td>\n      <td>1</td>\n      <td>0.986539</td>\n      <td>0.630117</td>\n      <td>19.376685</td>\n      <td>27.088447</td>\n      <td>0.01</td>\n      <td>1000</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>13</td>\n      <td>1</td>\n      <td>2.142883</td>\n      <td>0.241983</td>\n      <td>12.733782</td>\n      <td>19.742868</td>\n      <td>0.01</td>\n      <td>10000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>14</td>\n      <td>1</td>\n      <td>2.226118</td>\n      <td>0.209117</td>\n      <td>10.196291</td>\n      <td>19.131536</td>\n      <td>0.01</td>\n      <td>10000</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>15</td>\n      <td>1</td>\n      <td>2.146939</td>\n      <td>0.198500</td>\n      <td>9.796244</td>\n      <td>18.588745</td>\n      <td>0.01</td>\n      <td>10000</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>16</td>\n      <td>1</td>\n      <td>2.129873</td>\n      <td>0.203933</td>\n      <td>9.587353</td>\n      <td>17.945299</td>\n      <td>0.01</td>\n      <td>10000</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>17</td>\n      <td>1</td>\n      <td>2.142754</td>\n      <td>0.208950</td>\n      <td>10.874910</td>\n      <td>21.091576</td>\n      <td>0.01</td>\n      <td>10000</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>18</td>\n      <td>1</td>\n      <td>2.131707</td>\n      <td>0.229233</td>\n      <td>14.592563</td>\n      <td>28.288604</td>\n      <td>0.01</td>\n      <td>10000</td>\n      <td>16</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "params = OrderedDict(\n",
    "    lr = [.01],\n",
    "    batch_size = [100,1000, 10000], \n",
    "    num_workers = [0,1,2,4,8,16]  \n",
    ")\n",
    "m = RunManager()\n",
    "\n",
    "for run in RunBuilder.get_runs(params):\n",
    "    \n",
    "    network = Network() \n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size = run.batch_size, num_workers = run.num_workers) #num_workers is added \n",
    "    optimizer = optim.Adam(network.parameters(), lr=run.lr)\n",
    "\n",
    "    m.begin_run(run, network, train_loader)\n",
    "\n",
    "    for epoch in range(1):\n",
    "        m.begin_epoch()\n",
    "        for batch in train_loader:\n",
    "\n",
    "                images, labels = batch \n",
    "                preds = network(images) \n",
    "                loss = F.cross_entropy(preds, labels) \n",
    "                optimizer.zero_grad()               \n",
    "                loss.backward()          \n",
    "                optimizer.step()\n",
    "                \n",
    "                m.track_loss(loss)\n",
    "                m.track_num_correct(preds, labels)\n",
    "\n",
    "        m.end_epoch()\n",
    "    m.end_run()\n",
    "\n",
    "m.save('results')"
   ]
  }
 ]
}