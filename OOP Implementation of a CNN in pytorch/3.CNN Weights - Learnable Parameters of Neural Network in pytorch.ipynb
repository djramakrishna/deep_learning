{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599997849833",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Weights - Learnable Parameters in PyTorch Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "  class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "\n",
    "    def forward(self, t):\n",
    "        # implement the forward pass\n",
    "        return t\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyp par are picked arbitrarily\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learnable parameter - learned during the training process, \n",
    "With learnable parameters, we typically start out with a set of arbitrary values, and these values then get updated in an iterative fashion as the network learns.\n",
    "\n",
    "In fact, when we say that a network is learning, we specifically mean that the network is learning the appropriate values for the learnable parameters. Appropriate values are values that minimize the loss function.\n",
    "\n",
    "When it comes to our network, we might be thinking, where are these learnable parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learnable parameters are the weights inside a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Network(\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=192, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=60, bias=True)\n  (out): Linear(in_features=60, out_features=10, bias=True)\n)\n"
    }
   ],
   "source": [
    "print(network) #gives a string representaion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch what happens if we stop extending the neural network module class. print(network)\n",
    "# output is - <__main__.Network object at 0x0000017802302FD0>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "  class Network():\n",
    "    def __init__(self):\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "\n",
    "    def forward(self, t):\n",
    "        # implement the forward pass\n",
    "        return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<__main__.Network object at 0x000002385093CEB0>\n"
    }
   ],
   "source": [
    "network = Network()\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For this reason, in object oriented programming, we usually want to provide a string representation of our object inside our classes so that we get useful information when the object is printed. This string representation comes from Python’s default base class called object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overriding - we can override existing functionality after we extend a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    " class Network(nn.Module):\n",
    "     \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "\n",
    "    def forward(self, t):\n",
    "        # implement the forward pass\n",
    "        return t\n",
    "    \n",
    "    def __repr__(self): #repr - representation, we can change the string fucntion for represention \n",
    "        return \"lizardnet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "lizardnet\n"
    }
   ],
   "source": [
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "     \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120) #we need to flatten for linear so 12 but why 4*4 ? 4*4 is the last size of  the image pass through the 2 layer CNN\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "\n",
    "\n",
    "#For the convolutional layers, the kernel_size argument is a Python tuple (5,5) even though we only passed the number 5 in the constructor.\n",
    "#This is because our filters actually have a height and width, and when we pass a single number, the code inside the layer’s constructor assumes that we want a square filter.\n",
    "\n",
    "#The stride tells the conv layer how far the filter should slide after each operation in the overall convolution. This tuple says to slide by one unit when moving to the right and also by one unit when moving down.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Network(\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=192, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=60, bias=True)\n  (out): Linear(in_features=60, out_features=10, bias=True)\n)\n"
    }
   ],
   "source": [
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))"
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    " network.conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "network.conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Linear(in_features=192, out_features=120, bias=True)"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "network.fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Linear(in_features=120, out_features=60, bias=True)"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "network.fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Linear(in_features=60, out_features=10, bias=True)"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "network.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Network(\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=192, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=60, bias=True)\n  (out): Linear(in_features=60, out_features=10, bias=True)\n)"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing weights in the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Parameter containing:\ntensor([[[[-0.1599,  0.1478,  0.0639,  0.1777,  0.1237],\n          [-0.1889,  0.0113,  0.0425,  0.0919,  0.1709],\n          [-0.0376,  0.1057, -0.1515,  0.0692,  0.0162],\n          [-0.1468, -0.0213, -0.0059,  0.0702, -0.1406],\n          [ 0.0336, -0.1803, -0.0336, -0.1287,  0.0335]]],\n\n\n        [[[-0.0135,  0.0349, -0.1384, -0.0711, -0.0389],\n          [ 0.1969, -0.1428, -0.0893, -0.0895, -0.0818],\n          [ 0.1853, -0.1297, -0.0136,  0.1117, -0.0759],\n          [-0.1582, -0.0573,  0.0216,  0.1188,  0.0395],\n          [ 0.0063,  0.1840,  0.0952,  0.1686, -0.1601]]],\n\n\n        [[[ 0.0955, -0.1780,  0.1662,  0.1536, -0.1353],\n          [ 0.0230,  0.1392, -0.0136, -0.0940,  0.1962],\n          [ 0.1381, -0.1919,  0.0146,  0.0108, -0.1311],\n          [ 0.0095,  0.1662,  0.1009,  0.0139,  0.1424],\n          [-0.0072,  0.0633,  0.0392, -0.1529, -0.0486]]],\n\n\n        [[[-0.1534,  0.1206, -0.1600, -0.1073, -0.1282],\n          [ 0.1274,  0.1842, -0.0561, -0.1541, -0.0237],\n          [-0.1817, -0.0824, -0.1279, -0.0906,  0.1428],\n          [ 0.0891, -0.0817, -0.0010,  0.0553, -0.1023],\n          [-0.1190,  0.0401, -0.0725,  0.1991, -0.1075]]],\n\n\n        [[[-0.0677,  0.0050, -0.1132,  0.1508,  0.1972],\n          [-0.1402, -0.1575,  0.1946, -0.0959, -0.0701],\n          [-0.1834,  0.0624,  0.1263, -0.0410,  0.0565],\n          [-0.0682,  0.0403, -0.0472, -0.1711,  0.0255],\n          [-0.1871,  0.1221,  0.1464,  0.1241, -0.1098]]],\n\n\n        [[[ 0.0795, -0.0226,  0.0918,  0.1622,  0.1518],\n          [ 0.1421, -0.1080, -0.0507, -0.1676, -0.0081],\n          [-0.1709, -0.0049, -0.1722,  0.0893, -0.0141],\n          [ 0.0981,  0.1946,  0.1689, -0.1809,  0.1765],\n          [ 0.0605,  0.0229,  0.0471, -0.1811,  0.0958]]]], requires_grad=True)"
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "network.conv1.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter class extends tensor class and wt tensor is instance of parameter class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([6, 1, 5, 5])"
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "network.conv1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in above 6 - 6 filters \n",
    "# 1 - accounts for single input channel\n",
    "# 5 5 - accounts for h and weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([12, 6, 5, 5])"
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "network.conv2.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "\n",
    "This convolved one input channel with 6 filters of size 5*5\n",
    "\n",
    "All the filters are represented by a weight tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([12, 6, 5, 5])"
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "network.conv2.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12 filters - filter has a depth that matches the no of channels and its able to slide all channels in one go\n",
    "\n",
    "6 input channels - assumed as giving some depth to each of the filters \n",
    "\n",
    "1) All filters are rep using single tensor\n",
    "2) Filters have depth that accounts for color channels(inp channels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight tensor shape -\n",
    "#\n",
    "first axis : no of filters \n",
    "#\n",
    "second axis : depth of filters corr to input channels \n",
    "#\n",
    "thid and fourth - height and width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([6, 5, 5])"
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "network.conv2.weight[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the above gives a single filter (h , w = 5 and depth = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight tensor for linear/FC layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Network(\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=192, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=60, bias=True)\n  (out): Linear(in_features=60, out_features=10, bias=True)\n)"
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([120, 192])"
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "network.fc1.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rank 2 tensor as weight tensor \n",
    "#\n",
    "Height - desired output feature \n",
    "# \n",
    "width - lenght of input feature \n",
    "\n",
    "#\n",
    "Weight matrix in FC maps a vector space of 4d to 3d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([60, 120])"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "network.fc2.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([10, 60])"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "network.out.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = torch.tensor([1,2,3,4], dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_matrix = torch.tensor([[1,2,3,4],[2,3,4,5],[3,4,5,6]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([30., 40., 50.])"
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "weight_matrix.matmul(in_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#access all param at once "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([6, 1, 5, 5])\ntorch.Size([6])\ntorch.Size([12, 6, 5, 5])\ntorch.Size([12])\ntorch.Size([120, 192])\ntorch.Size([120])\ntorch.Size([60, 120])\ntorch.Size([60])\ntorch.Size([10, 60])\ntorch.Size([10])\n"
    }
   ],
   "source": [
    "for param in network.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "conv1.weight \t\t torch.Size([6, 1, 5, 5])\nconv1.bias \t\t torch.Size([6])\nconv2.weight \t\t torch.Size([12, 6, 5, 5])\nconv2.bias \t\t torch.Size([12])\nfc1.weight \t\t torch.Size([120, 192])\nfc1.bias \t\t torch.Size([120])\nfc2.weight \t\t torch.Size([60, 120])\nfc2.bias \t\t torch.Size([60])\nout.weight \t\t torch.Size([10, 60])\nout.bias \t\t torch.Size([10])\n"
    }
   ],
   "source": [
    "for name, param in network.named_parameters():\n",
    "    print(name,'\\t\\t', param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}